{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Melee.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVi3vTEgr9m4",
        "outputId": "675f40a8-10b8-4e36-bc2c-3febb50a9259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.12.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.42.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.37.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.3.1 keras-rl2 pygame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjYdKt2lsC3q",
        "outputId": "85149a2f-a449-4494-d456-7a08ba4bac4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.7.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Player1(pygame.sprite.Sprite):\n",
        "    def __init__(self, SCREEN_WIDTH, SCREEN_HEIGHT, model, is_cpu):\n",
        "        super(Player1, self).__init__()\n",
        "        self.SCREEN_WIDTH = SCREEN_WIDTH\n",
        "        self.SCREEN_HEIGHT = SCREEN_HEIGHT\n",
        "        self.model = model\n",
        "        self.is_cpu = is_cpu\n",
        "        self.surf = pygame.Surface((25, 25))\n",
        "        self.cooldown = 15\n",
        "        self.health = 10\n",
        "        self.surf.fill((255, 0, 0))\n",
        "        self.rect = self.surf.get_rect()\n",
        "\n",
        "    # Move the sprite based on user keypresses\n",
        "    def update(self, player, features, action=None):\n",
        "        upperY = self.rect.top\n",
        "        lowerY = self.rect.bottom\n",
        "        upperX = self.rect.right\n",
        "        lowerX = self.rect.left\n",
        "        hit = 0\n",
        "\n",
        "        if self.is_cpu:\n",
        "            if features is not None:\n",
        "                features = np.expand_dims(features, axis=0)\n",
        "                res = self.model.predict(np.asarray(features))\n",
        "                action = np.argmax(res)\n",
        "\n",
        "        if action == 0 and upperY > 0:\n",
        "            self.rect.move_ip(0, -10)\n",
        "        if action == 1 and lowerY < self.SCREEN_HEIGHT:\n",
        "            self.rect.move_ip(0, 10)\n",
        "        if action == 2 and lowerX > 0:\n",
        "            self.rect.move_ip(-10, 0)\n",
        "        if action == 3 and upperX < self.SCREEN_WIDTH:\n",
        "            self.rect.move_ip(10, 0)\n",
        "        if action == 4 and self.cooldown == 15:\n",
        "            self.cooldown = 0\n",
        "            player_pos = self.rect.center\n",
        "            other_player_pos = player.rect.center\n",
        "            x_diff = abs(player_pos[0] - other_player_pos[0])\n",
        "            y_diff = abs(player_pos[1] - other_player_pos[1])\n",
        "            hit = 2\n",
        "            if x_diff < 50 and y_diff < 50:\n",
        "                player.health -= 1\n",
        "                player.surf.fill((255,255,255))\n",
        "                hit = 1\n",
        "\n",
        "        if self.cooldown < 15:\n",
        "            self.cooldown += 1\n",
        "        return hit\n",
        "\n",
        "class Player2(pygame.sprite.Sprite):\n",
        "    def __init__(self, SCREEN_WIDTH, SCREEN_HEIGHT, model, is_cpu):\n",
        "        super(Player2, self).__init__()\n",
        "        self.SCREEN_WIDTH = SCREEN_WIDTH\n",
        "        self.SCREEN_HEIGHT = SCREEN_HEIGHT\n",
        "        self.model = model\n",
        "        self.is_cpu = is_cpu\n",
        "        self.surf = pygame.Surface((25, 25))\n",
        "        self.cooldown = 15\n",
        "        self.health = 10\n",
        "        self.surf.fill((0, 0, 255))\n",
        "        self.rect = self.surf.get_rect()\n",
        "        self.rect.move_ip(SCREEN_WIDTH-25, 0)\n",
        "\n",
        "    # Move the sprite based on user keypresses\n",
        "    def update(self, player, features, action=None):\n",
        "        upperY = self.rect.top\n",
        "        lowerY = self.rect.bottom\n",
        "        upperX = self.rect.right\n",
        "        lowerX = self.rect.left\n",
        "        hit = 0\n",
        "\n",
        "        # action = random.choice([0, 1, 2, 3, 4, 5])\n",
        "        # action = 4\n",
        "        if self.is_cpu:\n",
        "            if features is not None:\n",
        "                features = np.expand_dims(features, axis=0)\n",
        "                res = self.model.predict(np.asarray(features))\n",
        "                action = np.argmax(res)\n",
        "        \n",
        "        if action == 0 and upperY > 0:\n",
        "            self.rect.move_ip(0, -10)\n",
        "        elif action == 1 and lowerY < self.SCREEN_HEIGHT:\n",
        "            self.rect.move_ip(0, 10)\n",
        "        elif action == 2 and upperX < self.SCREEN_WIDTH:\n",
        "            self.rect.move_ip(10, 0)\n",
        "        elif action == 3 and lowerX > 0:\n",
        "            self.rect.move_ip(-10, 0)\n",
        "        elif action == 4 and self.cooldown == 15:\n",
        "            self.cooldown = 0\n",
        "            player_pos = self.rect.center\n",
        "            other_player_pos = player.rect.center\n",
        "            x_diff = abs(player_pos[0] - other_player_pos[0])\n",
        "            y_diff = abs(player_pos[1] - other_player_pos[1])\n",
        "            hit = 2\n",
        "            if x_diff < 50 and y_diff < 50:\n",
        "                player.surf.fill((255,255,255))\n",
        "                player.health -= 1\n",
        "                hit = 1\n",
        "\n",
        "        if self.cooldown < 15:\n",
        "            self.cooldown += 1\n",
        "        return hit\n",
        "\n",
        "class Melee():\n",
        "    def __init__(self, models, is_cpu):\n",
        "        self.SCREEN_WIDTH = 500 // 2\n",
        "        self.SCREEN_HEIGHT = 300 // 2\n",
        "        self.is_game_over = False\n",
        "        self.attack_range = 50\n",
        "        self.models = models\n",
        "        self.is_cpu = is_cpu\n",
        "\n",
        "        self.past_frames = []\n",
        "\n",
        "        self.player1 = Player1(self.SCREEN_WIDTH, self.SCREEN_HEIGHT, models[0], is_cpu[0])\n",
        "        self.player2 = Player2(self.SCREEN_WIDTH, self.SCREEN_HEIGHT, models[1], is_cpu[1])\n",
        "\n",
        "        self.all_sprites = pygame.sprite.Group()\n",
        "        self.players = pygame.sprite.Group()\n",
        "        self.all_sprites.add(self.player1)\n",
        "        self.all_sprites.add(self.player2)\n",
        "        self.players.add(self.player1)\n",
        "        self.players.add(self.player2)\n",
        "\n",
        "    def is_in_range(self):\n",
        "        player_pos = self.player1.rect.center\n",
        "        other_player_pos = self.player2.rect.center\n",
        "        x_diff = abs(player_pos[0] - other_player_pos[0])\n",
        "        y_diff = abs(player_pos[1] - other_player_pos[1])\n",
        "        if x_diff < 50 and y_diff < 50:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def game_over(self):\n",
        "        return self.is_game_over\n",
        "\n",
        "    def get_score(self):\n",
        "        return self.player1.health\n",
        "\n",
        "    def get_env(self):\n",
        "        features = []\n",
        "        features.append((self.player1.rect.centerx - self.player2.rect.centerx) / self.SCREEN_WIDTH)\n",
        "        features.append((self.player1.rect.centery - self.player2.rect.centery) / self.SCREEN_HEIGHT)\n",
        "        features.append(self.player1.rect.centerx / self.SCREEN_WIDTH)\n",
        "        features.append(self.player2.rect.centerx / self.SCREEN_WIDTH)\n",
        "        features.append(self.player1.rect.centery / self.SCREEN_HEIGHT)\n",
        "        features.append(self.player2.rect.centery / self.SCREEN_HEIGHT)\n",
        "\n",
        "        features.append(self.is_in_range())\n",
        "        features.append(self.player1.cooldown / 15)\n",
        "        features.append(self.player2.cooldown / 15)\n",
        "        return features\n",
        "\n",
        "    def get_features(self):\n",
        "        features = self.get_env()\n",
        "        num_past_frames = 8\n",
        "        if len(self.past_frames) == num_past_frames:\n",
        "            self.past_frames.pop(0)\n",
        "        self.past_frames.append(features)\n",
        "        if len(self.past_frames) == num_past_frames:\n",
        "            return self.past_frames\n",
        "        return None\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        features = self.get_features()\n",
        "        is_damaged = self.player1.update(self.player2, features)\n",
        "        is_dealt = self.player2.update(self.player1, features, action)\n",
        "\n",
        "        if is_dealt == 1:\n",
        "          reward += 50\n",
        "        elif is_damaged == 1:\n",
        "          reward -= 50\n",
        "        # elif self.is_in_range() == 1:\n",
        "        #   reward += 0.01\n",
        "        # else:\n",
        "        #   reward -= 0.001\n",
        "\n",
        "        if self.player1.health == 0 or self.player2.health == 0:\n",
        "          self.is_game_over = True\n",
        "\n",
        "        if self.player1.cooldown == 15:\n",
        "            self.player2.surf.fill((0, 0, 255))\n",
        "        if self.player2.cooldown == 15:\n",
        "            self.player1.surf.fill((255, 0, 0))\n",
        "\n",
        "        done = self.is_game_over \n",
        "        info = {}\n",
        "        return self.get_env(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.is_game_over = False\n",
        "        self.past_frames = []\n",
        "\n",
        "        for sprite in self.all_sprites:\n",
        "          sprite.kill()\n",
        "\n",
        "        self.player1 = Player1(self.SCREEN_WIDTH, self.SCREEN_HEIGHT, self.models[0], self.is_cpu[0])\n",
        "        self.player2 = Player2(self.SCREEN_WIDTH, self.SCREEN_HEIGHT, self.models[1], self.is_cpu[1])\n",
        "        self.all_sprites = pygame.sprite.Group()\n",
        "        self.players = pygame.sprite.Group()\n",
        "        self.all_sprites.add(self.player1)\n",
        "        self.all_sprites.add(self.player2)\n",
        "        self.players.add(self.player1)\n",
        "        self.players.add(self.player2)\n",
        "        return self.get_env()\n",
        "\n",
        "    def render(self):\n",
        "      pass\n",
        "\n",
        "    def close(self):\n",
        "      pass"
      ],
      "metadata": {
        "id": "yJTU02zUsFtE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model, dqn, game"
      ],
      "metadata": {
        "id": "BgxcU6IusIGb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('gen_3')\n",
        "models = [model, None]\n",
        "is_cpu = [True, False]\n",
        "game = Melee(models, is_cpu)\n",
        "\n",
        "num_classes = 6\n",
        "window_length = 8\n",
        "num_features = 9\n",
        "\n",
        "input = Input(shape=(window_length, num_features,))\n",
        "x = input\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "x = Dense(8, activation=\"relu\")(x)\n",
        "x = Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = Model(inputs=input, outputs=x)"
      ],
      "metadata": {
        "id": "v-OksagksJsc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent, NAFAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ],
      "metadata": {
        "id": "Qbw_P0IIsLlc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=0.1, value_test=.2, nb_steps=200000)\n",
        "memory = SequentialMemory(limit=1000, window_length=window_length)\n",
        "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=num_classes, nb_steps_warmup=1000, enable_dueling_network=True, dueling_type='avg')\n",
        "dqn.compile(tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "dqn.fit(game, nb_steps=200000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T_UHDkNsM_u",
        "outputId": "1f086053-2515-4a31-8cc8-c301686acc8d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 200000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 168s 17ms/step - reward: -0.5750\n",
            "30 episodes - episode_reward: -188.333 [-400.000, 0.000] - loss: 48.533 - mean_q: 1.194 - mean_eps: 0.975\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 186s 19ms/step - reward: -0.4900\n",
            "26 episodes - episode_reward: -186.538 [-450.000, 0.000] - loss: 22.342 - mean_q: 2.340 - mean_eps: 0.933\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: -0.4800\n",
            "37 episodes - episode_reward: -133.784 [-450.000, 50.000] - loss: 20.298 - mean_q: 6.662 - mean_eps: 0.888\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 183s 18ms/step - reward: -0.4150\n",
            "36 episodes - episode_reward: -112.500 [-400.000, 100.000] - loss: 19.934 - mean_q: 7.529 - mean_eps: 0.843\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -0.5400\n",
            "38 episodes - episode_reward: -143.421 [-300.000, 50.000] - loss: 19.858 - mean_q: 10.186 - mean_eps: 0.798\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -0.3650\n",
            "37 episodes - episode_reward: -100.000 [-300.000, 200.000] - loss: 18.322 - mean_q: 11.665 - mean_eps: 0.753\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: -0.3950\n",
            "34 episodes - episode_reward: -116.176 [-400.000, 100.000] - loss: 16.314 - mean_q: 13.545 - mean_eps: 0.708\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -0.2000\n",
            "37 episodes - episode_reward: -54.054 [-300.000, 250.000] - loss: 15.597 - mean_q: 14.953 - mean_eps: 0.663\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: -0.1800\n",
            "36 episodes - episode_reward: -43.056 [-400.000, 200.000] - loss: 16.623 - mean_q: 16.017 - mean_eps: 0.618\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -0.1750\n",
            "30 episodes - episode_reward: -66.667 [-400.000, 250.000] - loss: 15.468 - mean_q: 16.990 - mean_eps: 0.573\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: -0.1900\n",
            "30 episodes - episode_reward: -63.333 [-350.000, 350.000] - loss: 15.212 - mean_q: 18.441 - mean_eps: 0.528\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0200\n",
            "34 episodes - episode_reward: 7.353 [-400.000, 300.000] - loss: 16.709 - mean_q: 21.707 - mean_eps: 0.483\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.5450\n",
            "37 episodes - episode_reward: 147.297 [-350.000, 350.000] - loss: 14.513 - mean_q: 25.331 - mean_eps: 0.438\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 168s 17ms/step - reward: -0.0050\n",
            "2 episodes - episode_reward: -25.000 [-150.000, 100.000] - loss: 3.435 - mean_q: 29.108 - mean_eps: 0.393\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0000e+00\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0000e+00\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 185s 18ms/step - reward: 0.0000e+00\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 190s 19ms/step - reward: 0.3550\n",
            "37 episodes - episode_reward: 87.838 [-300.000, 450.000] - loss: 32.903 - mean_q: 41.866 - mean_eps: 0.213\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 188s 19ms/step - reward: 0.3200\n",
            "31 episodes - episode_reward: 109.677 [-100.000, 350.000] - loss: 20.223 - mean_q: 41.618 - mean_eps: 0.168\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 187s 19ms/step - reward: 0.7000\n",
            "done, took 3552.758 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f54a9c0d850>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(game, nb_episodes=5, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "6fAWIc_6sVA_",
        "outputId": "abc7469d-99c5-42d3-d72c-2516904c2249"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-6edef1af08de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3788\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3790\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3791\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m   \"\"\"\n\u001b[0;32m--> 627\u001b[0;31m   \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_input_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_get_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;31m# graph, create and cache a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     if (getattr(_SESSION, 'session', None) is None or\n\u001b[0;32m--> 591\u001b[0;31m         _SESSION.session.graph is not _current_graph(op_input_list)):\n\u001b[0m\u001b[1;32m    592\u001b[0m       \u001b[0;31m# If we are creating the Session inside a tf.distribute.Strategy scope,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m       \u001b[0;31m# we ask the strategy for the right session options to use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;34m\"\"\"The graph that was launched in this session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 5\n",
        "model = keras.models.load_model('gen_1')\n",
        "models = [model, None]\n",
        "is_cpu = [True, False]\n",
        "game = Melee(models, is_cpu)\n",
        "for episode in range(1, episodes+1):\n",
        "    state = game.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    steps = 0\n",
        "    while not done:\n",
        "        game.render()\n",
        "        action = random.choice([0,1,2,3,4,5])\n",
        "        # action = 1\n",
        "        n_state, reward, done, info = game.step(action)\n",
        "        score+=reward\n",
        "        steps += 1\n",
        "    print('Episode:{} Score:{} Steps:{}'.format(episode, score, steps))\n",
        "\n",
        "game.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk-hwonasXhs",
        "outputId": "d55e9792-fc00-48b2-90e7-836d97d56731"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:-200 Steps:539\n",
            "Episode:2 Score:0 Steps:95\n",
            "Episode:3 Score:-200 Steps:138\n",
            "Episode:4 Score:-200 Steps:922\n",
            "Episode:5 Score:-150 Steps:251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = dqn.model\n",
        "model.save('SavedKerasWeights/gen_4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Ev2zUzsZIW",
        "outputId": "de229523-7c1e-4fc8-8b6b-dc47a285e38b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: SavedKerasWeights/gen_4/assets\n"
          ]
        }
      ]
    }
  ]
}